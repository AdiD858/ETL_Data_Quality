{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8097551c-020b-40b8-9ee4-9b7ade9cbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.window import Window \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a8780-40ae-4934-8b8b-0d62ea149fc4",
   "metadata": {},
   "source": [
    "## Completeness tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f2a27d-7f30-4871-9110-5b9c35de83fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### PySpark check_nulls function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3692c21-8ee3-4f10-adc2-8a054009683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks if the DataFrame required column contains any null values\n",
    "\n",
    "def check_nulls(df,column_name):\n",
    "    logs = \"\"\n",
    "    null_count = df.filter(col(column_name).isNull()).count() # counting the null values\n",
    "    if null_count > 0:\n",
    "        logs += f\"Column '{column_name}' contains {null_count} null value(s).\"\n",
    "        return True, logs\n",
    "    else:\n",
    "        logs += f\"Column '{column_name}' does not contain any null values.\"\n",
    "        return False, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625ac3f-7492-4a3a-ac81-6df0362854c0",
   "metadata": {},
   "source": [
    "### PySaprk count_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd27131e-44d0-40cd-a53f-934e3a1ac4d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the number of records in the file are according to the expected number\n",
    "\n",
    "def count_records(df, expected_records_num):\n",
    "    \"\"\"\n",
    "    :param: df: spark data frame of the parquet file\n",
    "    :param: expected_records_num: expected number of records\n",
    "    \"\"\"\n",
    "    logs = \"\"  # Initialize logs list\n",
    "    records_count = df.count()\n",
    "    logs+= f\"Num of records in the file: {records_count}\"\n",
    "\n",
    "    if records_count == expected_records_num:\n",
    "        logs+= f\"Success: the DataFrame has the expected number of records: {expected_records_num}\"\n",
    "        return True,logs\n",
    "    else:\n",
    "        logs+= f\"Mismatch: the DatatFrame has {records_count} records, but {expected_records_num} were expected.\"\n",
    "        return False, logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157942d5-de38-4cc1-a7e5-4d8834f738d8",
   "metadata": {},
   "source": [
    "## Uniqueness tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db947f-8bb1-4f20-911a-06102a4b6fb6",
   "metadata": {},
   "source": [
    "### PySpark check_duplicates (in columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3851865b-9492-4f89-8c92-a457e7cca89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if unique columns containing duplicated values, return boolean. If there are -return False and the number of duplicates \n",
    "\n",
    "def check_duplicates(df, key_columns):\n",
    "    logs_value = \"\"\n",
    "    \n",
    "    # Step 1: Group by the key columns and count occurrences\n",
    "    grouped_df = df.groupBy(key_columns).count()\n",
    "    \n",
    "    # Step 2: Filter rows where the count is greater than 1 (these are duplicates)\n",
    "    duplicates = grouped_df.filter(col(\"count\") > 1)\n",
    "    \n",
    "    # Check if any duplicates were found\n",
    "    if duplicates.count() > 0:\n",
    "        # Collect duplicate records for logging\n",
    "        duplicates_list = duplicates.collect()\n",
    "        logs_value += f\"Duplicate records found based on keys {key_columns}: {len(duplicates_list)} duplicates.\"\n",
    "        return True, logs_value\n",
    "    else:\n",
    "        logs_value += \"No duplicates found in the specified key columns.\"\n",
    "        return False, logs_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a67f4-26aa-4c75-bc47-0dab960f6d02",
   "metadata": {},
   "source": [
    "### PySpark duplicated_files_by_name_and_size (checking in the current directory and in its subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfbb9839-d943-4e64-b45f-42708383bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def duplicated_files_by_name_and_size():\n",
    "    logs = \"\"  \n",
    "    files_info = {}  # dictionary to store file names and sizes\n",
    "    directory_path = os.getcwd()  #  current working directory\n",
    "\n",
    "    # Walk through all files in the directory and its folders\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            \n",
    "            # Check if it's a file (it will be since os.walk() lists files)\n",
    "            file_size = os.path.getsize(file_path)  # Get file size\n",
    "\n",
    "            # If file name and size already exist in files_info, it's a duplicate\n",
    "            if (file_name, file_size) in files_info:\n",
    "                logs += f\"Duplicated file found: {file_name}, Size: {file_size} bytes, Path: {file_path}.\"\n",
    "            else:\n",
    "                files_info[(file_name, file_size)] = True\n",
    "\n",
    "    # If logs contain duplicates, print them\n",
    "    if logs:\n",
    "        return True, logs\n",
    "    else:\n",
    "        return False, \"No duplicate files found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4cb34-713a-40c8-9db4-8c499d2461e8",
   "metadata": {},
   "source": [
    "### PySpark find_duplicate_columns (even if the column names are diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cee233-5f25-423f-ac68-50e840de080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_column_checksum(df, col_name):\n",
    "    \"\"\"\n",
    "   checksum - compute sum of the hash values in each column .\n",
    "    \"\"\"\n",
    "    # comput sum in each column and perform hash\n",
    "    checksum = df.select(F.sum(F.hash(F.col(col_name))).alias(\"checksum\")).collect()[0][\"checksum\"]\n",
    "    return checksum\n",
    "\n",
    "def find_duplicate_columns(df):\n",
    "    \"\"\"\n",
    " scan all columns in df and return dictionary with checsum as key, and list with same checksum as the value.\n",
    " if the columns has same checksum - they are duplicated.\n",
    "    \"\"\"\n",
    "    checksums = {}\n",
    "    duplicates = {}\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        cs = compute_column_checksum(df, col_name)\n",
    "        if cs in checksums:\n",
    "            # אם כבר קיימת עמודה עם אותו checksum, נוסיף את השם לרשימה\n",
    "            duplicates.setdefault(cs, [checksums[cs]]).append(col_name)\n",
    "        else:\n",
    "            checksums[cs] = col_name\n",
    "\n",
    "    # סינון – רק אותם checksum שמופיע יותר מפעם אחת\n",
    "    duplicates = {cs: cols for cs, cols in duplicates.items() if len(cols) > 1}\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0205f8e-8b21-4091-bd47-11a344fb380c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #find_duplicate_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab5422-2bbb-40fa-9da2-643a01e8349d",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cfce1-ba41-4263-8d57-95f22e2ff40b",
   "metadata": {},
   "source": [
    "### PySpark check_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671da112-176e-428f-86ad-fed376d2fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Checks if values between two columns (from two different DataFrames) are equal for rows that share the same key.\n",
    "\n",
    "\n",
    "def check_consistency(df1, df2, key, column):\n",
    "    # Assign explicit aliases\n",
    "    df1_alias = df1.alias(\"df1\")\n",
    "    df2_alias = df2.alias(\"df2\")\n",
    "    \n",
    "    # Perform an inner join on the key\n",
    "    joined_df = df1_alias.join(df2_alias, df1_alias[key] == df2_alias[key])\n",
    "    \n",
    "    # Compare the column values using the explicit alias names\n",
    "    mismatches = joined_df.filter(col(f\"df1.{column}\") != col(f\"df2.{column}\"))\n",
    "    \n",
    "    mismatch_count = mismatches.count()\n",
    "    logs_value = \"\"\n",
    "    \n",
    "    if mismatch_count > 0:\n",
    "        logs_value += f\"Number of mismatched rows: {mismatch_count}\"\n",
    "        return False, logs_value\n",
    "    else:\n",
    "        logs_value += f\"Consistency check passed for column: {column}.\"\n",
    "        return True, logs_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713dc65-3ce9-4680-99d6-6a5f7536345e",
   "metadata": {},
   "source": [
    "### PySpark check_foreign_key​ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a47a2095-7137-485c-889d-a54b6d1c8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks whether all foreign key (FK) values in one column exist in a reference column of another DataFrame.\n",
    "\n",
    "def check_foreign_key(df_fk, column_fk, df_ref, ref_column):\n",
    "    logs_value = \"\"\n",
    "    # Collect reference column values as a list\n",
    "    ref_values = [row[ref_column] for row in df_ref.select(ref_column).collect()]\n",
    "    \n",
    "    # Filter rows where the foreign key is not in the reference values\n",
    "    invalid_values = df_fk.filter(~df_fk[column_fk].isin(ref_values))\n",
    "    \n",
    "    # Check for invalid foreign key values\n",
    "    invalid_count = invalid_values.count()  # Get the count of invalid FK values\n",
    "    \n",
    "    if invalid_count == 0:\n",
    "        logs_value += \"The foreign key values are valid.\"\n",
    "        return True, logs_value\n",
    "    else:\n",
    "        logs_value += f\"Invalid foreign key values in {column_fk} : {invalid_count} rows\"\n",
    "        return False, logs_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95877e8c-44f0-4969-9118-458c5e5d2625",
   "metadata": {},
   "source": [
    "### PySpark function: dtype_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4540302d-1548-4d83-907a-61549dd98ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values in a specified column are consistent with the expected data type.\n",
    "from pyspark.sql.types import DataType\n",
    "\n",
    "def dtype_consistency(df: DataFrame, column_name: str, expected_type: DataType) -> tuple:\n",
    "    \"\"\"\n",
    "    expected_type (DataType): The expected data type for the column (e.g., IntegerType(), StringType()).\n",
    "    \"\"\"\n",
    "    # Attempt to cast the column to the expected type and filter out rows where casting fails (null after casting)\n",
    "    invalid_count = df.filter(col(column_name).cast(expected_type).isNull() & col(column_name).isNotNull()).count()\n",
    "    \n",
    "    if invalid_count == 0:\n",
    "        print(f\"All values in column '{column_name}' are consistent with the expected data type {expected_type}.\")\n",
    "        return True, 0\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' contains {invalid_count} inconsistent values that don't match {expected_type}.\")\n",
    "        return False, invalid_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d38e66-f82c-49b7-971a-96968d769410",
   "metadata": {},
   "source": [
    "## Validity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986d7b1-b2a0-4f07-871d-a8c3cd644c3e",
   "metadata": {},
   "source": [
    "### PySpark validate_numeric_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4eda92-e770-440f-8cb1-5093dde5ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that numeric values fall within a required range of minimum and maximum\n",
    "\n",
    "def validate_numeric_range(df, column, min_value, max_value):\n",
    "    logs = \"\"\n",
    "    \n",
    "    # Filter for out-of-range values\n",
    "    out_of_range = df.filter((df[column] > max_value) | (df[column] < min_value))\n",
    "    out_of_range_count = out_of_range.count()\n",
    "    \n",
    "    # filters for below min and above max values\n",
    "    count_lower_min = df.filter(df[column] < min_value).count()\n",
    "    count_higher_max = df.filter(df[column] > max_value).count()\n",
    "    \n",
    "    # If no values are out of range\n",
    "    if out_of_range_count == 0:\n",
    "        logs += f\"All values in '{column}' are within the range [{min_value}, {max_value}].\"\n",
    "        return True, logs\n",
    "    else:\n",
    "        # Log details for out-of-range values\n",
    "        logs += (f\"{out_of_range_count} values in '{column}' are out of range. \"\n",
    "                    f\"Values below minimum ({min_value}): {count_lower_min},\"\n",
    "                    f\"values above maximum ({max_value}): {count_higher_max}.\")\n",
    "        return False, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9646eee-c553-4bad-a68c-43a876feb6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5b5646-0d5a-4b36-afab-edbdf0376779",
   "metadata": {},
   "source": [
    "### PySpark validate_values_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d4df273-1515-4513-835e-d2522a9b6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_values_length(df,column_name, data_type):\n",
    "    \n",
    "    \"\"\"    \n",
    "    Params:\n",
    "    data_type (str): The data type string ('CHAR(18)', 'DECIMAL(9,0)', 'INTEGER', 'SMALLINT').\n",
    "    column_name (str): The name of the column being validated\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if all values are valid, False if any invalid value is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    logs = \"\"  # Initialize log string\n",
    "    invalid_count = 0  # Count of invalid values\n",
    "\n",
    "    # Handle CHAR(x) type\n",
    "    if data_type.startswith('CHAR'):\n",
    "        max_length = int(data_type.split('(')[1].split(')')[0])\n",
    "        invalid_count = df.filter(length(col(column_name)) > max_length).count()\n",
    "\n",
    "    # Handle DECIMAL(x,y) type\n",
    "    elif data_type.startswith('DECIMAL'):\n",
    "        total_digits, decimal_places = map(int, data_type.split('(')[1].split(')')[0].split(','))\n",
    "        # Check the number of digits before and after the decimal point\n",
    "        invalid_count = df.filter(\n",
    "            (length(col(column_name).cast('string')) > total_digits) |\n",
    "            (col(column_name).cast('decimal').cast(f'decimal({total_digits},{decimal_places})').isNull())\n",
    "        ).count()\n",
    "\n",
    "    # Handle SMALLINT and INTEGER types\n",
    "    elif data_type in ['SMALLINT', 'INTEGER']:\n",
    "        # Check if the column has non-integer values\n",
    "        invalid_count = df.filter(~col(column_name).cast('int').isNotNull()).count()\n",
    "\n",
    "    # Add logs based on invalid count\n",
    "    if invalid_count == 0:\n",
    "        logs += f\"All values lengths in column '{column_name}' are within the expected length. \"\n",
    "        return True, logs\n",
    "    else:\n",
    "        logs += f\"Column '{column_name}' contains {invalid_count} invalid values.\"\n",
    "        return False, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e522d-250f-4ea4-9148-538e6b5f98af",
   "metadata": {},
   "source": [
    "### PySpark validate_allowed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4974a5c9-6101-4c23-8858-475437dabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values in a specified column are within the allowed values, regardless of data type.\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def validate_allowed_values(df: DataFrame, column_name: str, allowed_values: list) -> tuple:\n",
    "    \n",
    "    # Filter the DataFrame to find rows with invalid values\n",
    "    invalid_values_count = df.filter(~col(column_name).isin(allowed_values)).count()\n",
    "    \n",
    "    if invalid_values_count == 0:\n",
    "        print(f\"All values in column '{column_name}' are valid according to the allowed values: {allowed_values}.\")\n",
    "        return True, 0\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' contains {invalid_values_count} invalid values.\")\n",
    "        return False, invalid_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52265eaf-10c0-4280-b6f1-de4a8f422487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70f152-2eb3-4a52-baac-866573a63c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112d94b3-2bef-4be5-b1c9-bf6731fdb6c0",
   "metadata": {},
   "source": [
    "### PySpark validate_date_format (& timestamp values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd76c412-cd9f-40f9-ad83-41026035294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dtae values in a specific column are according to the expected format (date or timestamp)\n",
    "\n",
    "def validate_date_format(df, column, expected_format):\n",
    "    logs = \"\"\n",
    "\n",
    "    # Determine the expected format: date or timestamp \n",
    "    if \"HH\" in expected_format or \"mm\" in expected_format:  # if it's a timestamp format\n",
    "        converted_df = df.withColumn(\"converted_value\", to_timestamp(df[column], expected_format))\n",
    "        invalid_values = converted_df.filter(converted_df[\"converted_value\"].isNull())\n",
    "        invalid_count = invalid_values.count()\n",
    "\n",
    "        if invalid_count == 0:\n",
    "            logs += f\"All timestamp values in column '{column}' are in the expected format '{expected_format}'.\"\n",
    "            return True, logs\n",
    "        else:\n",
    "            logs += f\"There are {invalid_count} invalid timestamp values in column '{column}'.\"\n",
    "            return False, logs\n",
    "            \n",
    "    else:  # Assuming it's a date format if it's not a timestamp\n",
    "        converted_df = df.withColumn(\"converted_value\", to_date(df[column], expected_format))\n",
    "        invalid_values = converted_df.filter(converted_df[\"converted_value\"].isNull())\n",
    "        invalid_count = invalid_values.count()\n",
    "\n",
    "        if invalid_count == 0:\n",
    "            logs += f\"All date values in column '{column}' are in the expected format '{expected_format}'.\"\n",
    "            return True, logs\n",
    "        else:\n",
    "            logs += f\"There are {invalid_count} invalid date values in column '{column}'.\"\n",
    "            return False, logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b26337-5139-40df-a6f9-e128f97d2849",
   "metadata": {},
   "source": [
    "## Timeliness tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2de238-4420-4a5d-ad7e-47154189eda1",
   "metadata": {},
   "source": [
    "### PySpark file_timeliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4665e468-1b8b-417a-9e11-2cc9c3f2a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures the file is updated within the required hours threshold (by default 24 hours). \n",
    "\n",
    "def file_timeliness(file_path, hours_threshold=24):\n",
    "    logs= \"\"\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        return False, f\"File {file_path} does not exist.\"\n",
    "\n",
    "    # Get the current time\n",
    "    current_time = datetime.now()\n",
    "\n",
    "    # gets the last modified time of the file in seconds\n",
    "    last_modified_timestamp = os.path.getmtime(file_path)\n",
    "    last_modified_time = datetime.fromtimestamp(last_modified_timestamp)\n",
    "\n",
    "    # calculate the time difference\n",
    "    time_diff = current_time - last_modified_time \n",
    "\n",
    "    # check if the file is timely\n",
    "    if time_diff <= timedelta(hours=hours_threshold): #  create the threshold for how recent the file should be\n",
    "        logs += f\"File {file_path} is up to date. Last modified {last_modified_time}\"\n",
    "        return True, logs\n",
    "    else:\n",
    "        logs += f\"File {file_path} is stale. Last modified {last_modified_time} (older than {hours_threshold} hours.\"\n",
    "        return False, logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8391d-7a6d-4c51-9c1f-683846ce2f83",
   "metadata": {},
   "source": [
    "### PySpark validate_date_range (older\\future records than defined time )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03484018-5151-482f-8c8d-56cca6f0962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that data falls within a valid time range in the required column,by checking if there are older records than the current time or any future records \n",
    "\n",
    "def validate_date_range(df, column, duration, time_unit):\n",
    "    logs = \"\"\n",
    "    is_valid = True\n",
    "\n",
    "    # Create a dynamic threshold for late records\n",
    "    late_threshold_expr = f\"current_timestamp() - INTERVAL {duration} {time_unit.upper()}\"\n",
    "\n",
    "    # Filter late and future records: expr() function to allow sql interval expression in pyspark dataframe\n",
    "    late_records_count = df.filter(col(column) < expr(late_threshold_expr)).count()\n",
    "    future_records_count = df.filter(col(column) > current_timestamp()).count()\n",
    "\n",
    "    # Check late records\n",
    "    if late_records_count > 0:\n",
    "        logs += f\"{late_records_count} late records found in column '{column}' older than {duration} {time_unit.lower()}.\"\n",
    "        is_valid = False\n",
    "    else:\n",
    "        logs += f\"No late records: The column '{column}' has no records older than {duration} {time_unit.lower()}.\"\n",
    "\n",
    "    # Check future records\n",
    "    if future_records_count > 0:\n",
    "        logs += f\"{future_records_count} future records found in column '{column}' beyond the current timestamp.\"\n",
    "        is_valid = False\n",
    "    else:\n",
    "        logs += f\"No future records: The column '{column}' has no records beyond the current timestamp.\"\n",
    "\n",
    "    return is_valid, logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae8db8-ac99-4fe1-90bf-5a5fb3369116",
   "metadata": {},
   "source": [
    "## Accuracy tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc8176-5043-41ca-8b4b-4374c18171e0",
   "metadata": {},
   "source": [
    "### PySpark validate_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fac46e3-6209-45c5-a90a-ebc8f0f16744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the sum of a specific column in the DF to the sum provided in the control file.\n",
    "\n",
    "\n",
    "def validate_sum(df, column, expected_sum):\n",
    "    logs = \"\"\n",
    "\n",
    "    actual_sum = df.select(spark_sum(column)).collect()[0][0]\n",
    "    logs += f\"Actual sum of '{column}': {actual_sum}\"\n",
    "\n",
    "    # Compare actual sum and expected sum\n",
    "    if actual_sum == expected_sum:\n",
    "        logs += f\"Result: The actual sum matches the expected sum: {expected_sum}\"\n",
    "        return True, logs\n",
    "    else:\n",
    "        logs += f\"Result: Expected sum {expected_sum}, but got {actual_sum}\"\n",
    "        return False, logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bfb47-edaa-4ddc-89d9-7206fdfb6638",
   "metadata": {},
   "source": [
    "### PySpark moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18f7abc1-8972-4d2e-8a8a-36e993241456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(df, column, partitioned_column,date_column, tolerance_deviation, window_size):\n",
    "    logs = \"\"  \n",
    "    \n",
    "    # window specification based on partitioned_column and date\n",
    "    window_spec = Window.partitionBy(partitioned_column).orderBy(F.col(date_column).cast('timestamp')).rowsBetween(-window_size, 0)\n",
    "    \n",
    "    # calculate moving average for the column\n",
    "    df = df.withColumn(\"moving_avg\", F.avg(F.col(column)).over(window_spec))\n",
    "    \n",
    "    # calculate deviation percentage from the moving average\n",
    "    df = df.withColumn(\n",
    "        \"deviation\",\n",
    "        F.when(\n",
    "            F.col(\"moving_avg\").isNotNull() & (F.col(\"moving_avg\") != 0),\n",
    "            (F.abs(F.col(column) - F.col(\"moving_avg\")) / F.col(\"moving_avg\")) * 100\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # check if any deviation exceeds the tolerance_deviation\n",
    "    max_deviation = df.agg(F.max(\"deviation\")).collect()[0][0]\n",
    "    \n",
    "    # return False if the maximum deviation exceeds the tolerance\n",
    "    if max_deviation > tolerance_deviation:\n",
    "        logs += f\"Significant deviation found in {column}. Maximum deviation: {max_deviation:.2f}%\"\n",
    "        return False, logs\n",
    "    else:\n",
    "        return True, \"No significant deviation found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "667c54ef-ac5b-4ff0-b0e6-3dd35c0deb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03049530-c93a-4a0b-b879-98c9883c6f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337241fb-0ce1-4941-a655-08d7c2bb1851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91d8d7d8-8130-4321-a30a-7ea24572e6f1",
   "metadata": {},
   "source": [
    "### testing functions names:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21a5e4-cad2-4ca7-aeaf-c52779e1bdd9",
   "metadata": {},
   "source": [
    "['check_consistency',\n",
    " 'check_duplicates',\n",
    " 'check_foreign_key',\n",
    " 'check_nulls',\n",
    " 'count_records',\n",
    " 'dtype_consistency',\n",
    " 'duplicated_files_by_name_and_size',\n",
    " 'duplicated_files_current_path',\n",
    " 'file_timeliness',\n",
    " 'moving_average',\n",
    " 'validate_allowed_values',\n",
    " 'validate_date_format',\n",
    " 'validate_date_range',\n",
    " 'validate_numeric_range',\n",
    " 'validate_sum',\n",
    " 'validate_values_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3635e-1e7e-4195-ac25-d2a4fe5d8836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f4e4a-68b2-4b9d-9cd3-47bde0090b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
