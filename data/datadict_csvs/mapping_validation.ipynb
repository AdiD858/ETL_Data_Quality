{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd73ebd8-b1b9-48b5-b745-b6a942b7bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, countDistinct,sum\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c36b9f-ba7d-428f-bad6-8f456fdfe6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"LoanDataQuality\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3cf3dff-45a0-4e8f-9020-10374db5a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans = spark.read.option('header','true').csv(\"LCloan_raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983fcd73-e0af-4a76-8d7b-dbf665716fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PySpark_testing import *\n",
    "from PySpark_testing import check_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a249f2dc-21b5-41fc-a7eb-f3e290b9f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mapping = pd.read_csv(\"full_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cdce2c-d583-4c8f-ac97-177e6931ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_mapping= pd.read_csv('focused_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "303fa2d4-6a99-4064-8958-69955ea985a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1            acc_open_past_24mths\n",
      "10       chargeoff_within_12_mths\n",
      "12     collections_12_mths_ex_med\n",
      "40           mo_sin_old_rev_tl_op\n",
      "52                   next_pymnt_d\n",
      "63               num_tl_120dpd_2m\n",
      "64                   num_tl_30dpd\n",
      "65             num_tl_90g_dpd_24m\n",
      "66             num_tl_op_past_12m\n",
      "68                    open_acc_6m\n",
      "72                    open_rv_12m\n",
      "73                    open_rv_24m\n",
      "122               hardship_status\n",
      "124               hardship_amount\n",
      "125           hardship_start_date\n",
      "126             hardship_end_date\n",
      "127       payment_plan_start_date\n",
      "128               hardship_length\n",
      "129                  hardship_dpd\n",
      "138               settlement_date\n",
      "141               settlement_term\n",
      "146      chargeoff_within_12_mths\n",
      "147    collections_12_mths_ex_med\n",
      "158          mo_sin_old_rev_tl_op\n",
      "173              num_tl_120dpd_2m\n",
      "174                  num_tl_30dpd\n",
      "175            num_tl_90g_dpd_24m\n",
      "176            num_tl_op_past_12m\n",
      "177                   open_acc_6m\n",
      "181                   open_rv_12m\n",
      "182                   open_rv_24m\n",
      "Name: column_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filtered_df = focused_mapping[focused_mapping['test_function'].apply(lambda x: 'validate_date_format' in x)]\n",
    "print(filtered_df['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4add71c7-0cd3-4471-959f-c631afbaf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('new.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "465b098b-68f3-4d55-a183-e7bf46b2fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa650b-d881-42be-adce-c4f0bf2edf92",
   "metadata": {},
   "source": [
    "### functions validations before running\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c1e178-7046-4a70-b166-8a1c6e54d9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 'Number of mismatched rows: 580119')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_consistency(df_loans,df2, column='acc_open_past_24mths', key='addr_state')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213fc613-e177-40e4-b941-4338981463c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_foreign_key(df_loans, 'acc_now_delinq', df2, 'addr_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5235da9a-9cd2-46c2-86f8-654a16fb0102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in column 'addr_state' are consistent with the expected data type string.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_consistency(df_loans,'addr_state', 'string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dddf48d8-06c9-4ee8-90bf-1649841f0edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " \"1826460 values in 'funded_amnt' are out of range. Values below minimum (2290): 50143,values above maximum (7000): 1776317.\")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_numeric_range(df_loans, 'funded_amnt', 2290, 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561dc24-dec0-4f96-b086-c48d608e27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_values_length(df,column_name, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7ae08-0f98-4097-b5b4-0c892974b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_allowed_values(df: DataFrame, column_name: str, allowed_values: list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5363530-ae82-4a07-a2c9-526925d32741",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o145.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 35.0 failed 1 times, most recent failure: Lost task 1.0 in stage 35.0 (TID 114) (10.0.0.5 executor driver): java.io.FileNotFoundException: C:\\Users\\user\\AppData\\Local\\Temp\\blockmgr-1700b353-ff52-4e86-a3cf-61342bed0e28\\05\\temp_shuffle_3a66de97-8e41-45c3-a6f5-a5608617b576 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:289)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:230)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\user\\AppData\\Local\\Temp\\blockmgr-1700b353-ff52-4e86-a3cf-61342bed0e28\\05\\temp_shuffle_3a66de97-8e41-45c3-a6f5-a5608617b576 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:289)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:230)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m validate_date_format(df_loans, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettlement_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd-m-yyyy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\ETL-Data-Quality-\\data\\datadict_csvs\\PySpark_testing.py:391\u001b[0m, in \u001b[0;36mvalidate_date_format\u001b[1;34m(df, column, expected_format)\u001b[0m\n\u001b[0;32m    389\u001b[0m converted_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_date(df[column], expected_format))\n\u001b[0;32m    390\u001b[0m invalid_values \u001b[38;5;241m=\u001b[39m converted_df\u001b[38;5;241m.\u001b[39mfilter(converted_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misNull())\n\u001b[1;32m--> 391\u001b[0m invalid_count \u001b[38;5;241m=\u001b[39m invalid_values\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    394\u001b[0m     logs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll date values in column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are in the expected format \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m \n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o145.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 35.0 failed 1 times, most recent failure: Lost task 1.0 in stage 35.0 (TID 114) (10.0.0.5 executor driver): java.io.FileNotFoundException: C:\\Users\\user\\AppData\\Local\\Temp\\blockmgr-1700b353-ff52-4e86-a3cf-61342bed0e28\\05\\temp_shuffle_3a66de97-8e41-45c3-a6f5-a5608617b576 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:289)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:230)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.io.FileNotFoundException: C:\\Users\\user\\AppData\\Local\\Temp\\blockmgr-1700b353-ff52-4e86-a3cf-61342bed0e28\\05\\temp_shuffle_3a66de97-8e41-45c3-a6f5-a5608617b576 (The system cannot find the path specified)\r\n\tat java.base/java.io.FileOutputStream.open0(Native Method)\r\n\tat java.base/java.io.FileOutputStream.open(FileOutputStream.java:289)\r\n\tat java.base/java.io.FileOutputStream.<init>(FileOutputStream.java:230)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.initialize(DiskBlockObjectWriter.scala:147)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:167)\r\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:330)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\n"
     ]
    }
   ],
   "source": [
    "validate_date_format(df_loans, 'settlement_date', 'dd-m-yyyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e8b91-e068-45b6-9128-1cd4fc44053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_timeliness(file_path, hours_threshold=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada11165-1cb2-4f63-9f17-0905bfff7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    " validate_date_range(df, column, duration, time_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20121969-1c06-4171-913c-94201f7836cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(df, column, partitioned_column,date_column, tolerance_deviation, window_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6dc5d70-6aed-4856-8b71-dcb4509c17ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'funded_amnt[0]'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2336e9b2-8cdd-41aa-8aba-f522e1f5ef7f",
   "metadata": {},
   "source": [
    "### creating the operational table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bacd7-2a28-4ab9-b90c-9fc69e282453",
   "metadata": {},
   "outputs": [],
   "source": [
    "operational = focused_mapping.withColumn('is_active'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c56c9b68-37ab-4fd4-adf5-02538697cae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_type</th>\n",
       "      <th>business_category</th>\n",
       "      <th>test_function</th>\n",
       "      <th>test_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc_now_delinq</td>\n",
       "      <td>string</td>\n",
       "      <td>credit_info</td>\n",
       "      <td>['count_records', 'validate_values_length', 'c...</td>\n",
       "      <td>['stability', 'completeness', 'credit_score_co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acc_open_past_24mths</td>\n",
       "      <td>int</td>\n",
       "      <td>dates</td>\n",
       "      <td>['validate_sum', 'count_records', 'check_dupli...</td>\n",
       "      <td>['stability', 'completeness', 'date_format_val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addr_state</td>\n",
       "      <td>string</td>\n",
       "      <td>loan_info</td>\n",
       "      <td>['count_records', 'validate_values_length', 'c...</td>\n",
       "      <td>['stability', 'completeness', 'accuracy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all_util</td>\n",
       "      <td>string</td>\n",
       "      <td>credit_info</td>\n",
       "      <td>['count_records', 'validate_values_length', 'c...</td>\n",
       "      <td>['stability', 'completeness', 'credit_score_co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>string</td>\n",
       "      <td>financial_metrics</td>\n",
       "      <td>['count_records', 'validate_values_length', 'm...</td>\n",
       "      <td>['stability', 'completeness']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            column_name column_type  business_category  \\\n",
       "0        acc_now_delinq      string        credit_info   \n",
       "1  acc_open_past_24mths         int              dates   \n",
       "2            addr_state      string          loan_info   \n",
       "3              all_util      string        credit_info   \n",
       "4            annual_inc      string  financial_metrics   \n",
       "\n",
       "                                       test_function  \\\n",
       "0  ['count_records', 'validate_values_length', 'c...   \n",
       "1  ['validate_sum', 'count_records', 'check_dupli...   \n",
       "2  ['count_records', 'validate_values_length', 'c...   \n",
       "3  ['count_records', 'validate_values_length', 'c...   \n",
       "4  ['count_records', 'validate_values_length', 'm...   \n",
       "\n",
       "                                           test_type  \n",
       "0  ['stability', 'completeness', 'credit_score_co...  \n",
       "1  ['stability', 'completeness', 'date_format_val...  \n",
       "2          ['stability', 'completeness', 'accuracy']  \n",
       "3  ['stability', 'completeness', 'credit_score_co...  \n",
       "4                      ['stability', 'completeness']  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focused_mapping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ccd62-ea10-443f-a812-eac39b6814bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38eb582-359b-4bab-b52d-366c4f67ddec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
